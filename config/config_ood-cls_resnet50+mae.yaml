## Config File

dataset: ROBIN  # dataset name
log_dir: logs/resnet50+mae_ood-cls_nore0.5_nobn_1.0mnceloss_buildtransform_colorjitter0.4  # path to log file
train_data: ../../../../dataset/ROBIN/processed_dataset/ROBINv1.1-cls-pose/train  # path to train dataset
val_data_iid: ../../../../dataset/ROBIN/processed_dataset/ROBINv1.1-cls-pose/iid_test  # path to iid validation dataset
val_data_nui: ../../../../dataset/ROBIN/processed_dataset/ROBINv1.1-cls-pose/nuisances  # path to nuisance validation dataset
val_data_final: ../../../../dataset/ROBIN/OOD-CV-phase2/phase2-cls  # path to final validation dataset
input_size: 224  # input image size
arch: resnet50 + mae  # model architecture (default: resnet50 or mae)
workers: 4  # number of data loading workers (default: 4)
epochs: 90  # number of total epochs to run
warmup_epochs: 5  # epochs to warmup lr
start_epoch: 0  # manual epoch number (useful on restarts)
batch_size: 128  # mini-batch size (default: 256), this is the total batch size of all GPUs on the current node when using Data Parallel or Distributed Data Parallel
lr: 0.0001  # initial learning rate (default: 0.0001)
min_lr: 0.000001  # lower lr bound for cyclic schedulers that hit 0 (default: 0.000001)
momentum: 0.9  # momentum
weight_decay: 0.0001  # weight decay (default: 0.0001)
betas: [0.9, 0.999]  # AdamW betas
print_freq: 10  # print frequency (default: 10)
pretrained: on  # use pre-trained model
pretrained_mae_path: ../mae_checkpoints/mae_visualize_vit_base.pth  # pre-trained model path
seed: off  # seed for initializing training
cutmix: off  # cutmix option
beta: 1.0  # hyperparameter beta for cutmix
cutmix_prob: 1.0  # cutmix probability
patch_aug: off  # patch level augmentation

